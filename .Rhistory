detectionStat %>% group_by(qcIterations, order) %>%
summarize(mean(TP_Cnt), mean(FP_Cnt))
summarize(mean(TP_Cnt), mean(FP_Cnt), mean(efficiencyRMSE))
detectionStat %>% group_by(qcIterations, order) %>%
summarize(mean(TP_Cnt), mean(FP_Cnt), mean(efficiencyRMSE))
colnames(detectionStat)
temp = cast(detectionStat, ID ~ qcIterations + order, value = "efficiencyRMSE", mean)
library(reshape2)
temp = cast(detectionStat, ID ~ qcIterations + order, value = "efficiencyRMSE", mean)
library(reshape)
temp = cast(detectionStat, ID ~ qcIterations + order, value = "efficiencyRMSE", mean)
head(temp)
all(temp[, 2] == temp[, 3])
all(temp[, 2] == temp[, 4])
all(temp[, 2] == temp[, 5])
summary(temp[, 2] - temp[, 3])
summary(temp[, 2] - temp[, 5])
summary(temp[, 3] - temp[, 5])
source("~/GitHub/Italian/getConjugations.R"
)
source("~/GitHub/Italian/parseOneTense.R")
getConjugation("dovere")
getConjugation
getConjugations("dovere")
library(dplyr)
getConjugations("dovere")
library(htmlr)
library(rvest)
getConjugations("dovere")
verbs = read.csv("~/GitHub/Italian/verbs.csv")
verbs
for(verb in verbs){
print(verb)
}
verbs = read.csv("~/GitHub/Italian/verbs.csv", stringsAsFactors = FALSE)
for(verb in verbs){
print(verb)
}
verbs = gsub(" *", "", verbs)
verbs
for(verb in verbs){
}
for(verb in verbs){
print(verb)
}
verbs = read.csv("~/GitHub/Italian/verbs.csv", stringsAsFactors = FALSE)
verbs = gsub(" ", "", verbs)
for(verb in verbs){
print(verb)
}
verbs = read.csv("~/GitHub/Italian/verbs.csv", stringsAsFactors = FALSE)
verbs
verbs = gsub("\w", "", verbs)
verbs = gsub("\t", "", verbs)
for(verb in verbs){
print(verb)
}
verbs = read.csv("~/GitHub/Italian/verbs.csv", stringsAsFactors = FALSE)
for(verb in verbs){
print(verb)
}
verbs[1]
verbs[1,1]
gsub(" ", "", verbs[1,1])
gsub("( )*", "", verbs[1,1])
gsub("\h*", "", verbs[1,1])
gsub("\t*", "", verbs[1,1])
gsub("\s*", "", verbs[1,1])
gsub("\m*", "", verbs[1,1])
verbs = gsub("[[:space:]]*", "", verbs[, 1])
verbs
verbs = read.csv("~/GitHub/Italian/verbs.csv", stringsAsFactors = FALSE)
verbs = gsub("[[:space:]]*", "", verbs[, 1])
for(verb in verbs){
print(verb)
}
newWords = try(getConjugations(verb))
verb = "dovere"
newWords = try(getConjugations(verb))
newWords
verbConjugations = NULL
newWords
newWords$verb = verb
newWords
newWords$infinitivo = verb
verbConjugations = NULL
for(verb in verbs){
print(verb)
newWords = try(getConjugations(verb))
if(!is(newWords, "try-error")){
newWords$infinitivo = verb
verbConjugations = rbind(verbConjugations, newWords)
}
save(verbConjugations, file = "~/GitHub/Italian/verbConjugations.RData")
print(dim(verbConjugations))
}
newWords
setwd("C:/Users/rockc_000/Documents/Professional Files/Mines/Research/Wind QC/")
library(stringr)
setwd("C:/Users/rockc_000/Documents/Professional Files/Mines/Research/Wind QC/")
files = dir("Results/Simulation_20150223/", pattern = "^Sim.*RData$",
full.names = TRUE)
rm(params)
rm(detection)
for(file in files){
load(file)
station_pressure = str_match( file, "[0-9]{5}_[0-9]{3}" )
station = str_match(station_pressure, "[0-9]{5}" )[1]
pressure = gsub("[0-9]{5}_", "", station_pressure )[1]
if(exists("params"))
params = rbind(params, dataStat)
else
params = dataStat
detectionStat$station = station
detectionStat$pressure = pressure
if(exists("detection"))
detection = rbind(detection, detectionStat)
else
detection = detectionStat
cat("File",file,"processed.\n")
}
head(detection)
library(dplyr)
colnames(detection)
detection %>% group_by(order, qcIterations, station, pressure) %>%
summarize(TP_Mean = mean(TP_Cnt), FP_Mean = mean(FP_Cnt), eff_Mean = mean(efficiencyRMSE))
detection %>% group_by(order, qcIterations, station, pressure) %>%
summarize(TP_Mean = mean(TP_Cnt), FP_Mean = mean(FP_Cnt), eff_Mean = mean(efficiencyRMSE))
detection %>% group_by(station, pressure, order, qcIterations) %>%
summarize(TP_Mean = mean(TP_Cnt), FP_Mean = mean(FP_Cnt), eff_Mean = mean(efficiencyRMSE))
detection %>% group_by(station, pressure, order, qcIterations) %>%
summarize(TP_Mean = mean(TP_Cnt), FP_Mean = mean(FP_Cnt),
eff_Mean = mean(efficiencyRMSE)) %>%
ggplot(data = ., aes(x = paste(order, qcIterations), y = TP_Mean))
library(dplyr)
library(ggplot2)
detection %>% group_by(station, pressure, order, qcIterations) %>%
summarize(TP_Mean = mean(TP_Cnt), FP_Mean = mean(FP_Cnt),
eff_Mean = mean(efficiencyRMSE)) %>%
ggplot(data = ., aes(x = paste(order, qcIterations), y = TP_Mean)) +
geom_point()
ggplot(data = ., aes(x = paste(order, qcIterations), y = TP_Mean)) +
geom_point() + geom_line(aes(group = paste(order, qcIterations)))
detection %>% group_by(station, pressure, order, qcIterations) %>%
summarize(TP_Mean = mean(TP_Cnt), FP_Mean = mean(FP_Cnt),
eff_Mean = mean(efficiencyRMSE)) %>%
ggplot(data = ., aes(x = paste(order, qcIterations), y = TP_Mean)) +
geom_point() + geom_line(aes(group = paste(order, qcIterations)))
detection %>% group_by(station, pressure, order, qcIterations) %>%
summarize(TP_Mean = mean(TP_Cnt), FP_Mean = mean(FP_Cnt),
eff_Mean = mean(efficiencyRMSE)) %>%
ggplot(data = ., aes(x = paste(order, qcIterations), y = TP_Mean)) +
geom_point() + geom_line(aes(group = paste(station, pressure)))
detection %>% group_by(station, pressure, order, qcIterations) %>%
summarize(TP_Mean = mean(TP_Cnt), FP_Mean = mean(FP_Cnt),
eff_Mean = mean(efficiencyRMSE)) %>%
ggplot(data = ., aes(x = paste(order, qcIterations), y = FP_Mean)) +
geom_point() + geom_line(aes(group = paste(station, pressure)))
detection %>% group_by(station, pressure, order, qcIterations) %>%
summarize(TP_Mean = mean(TP_Cnt), FP_Mean = mean(FP_Cnt),
eff_Mean = mean(efficiencyRMSE)) %>%
ggplot(data = ., aes(x = paste(order, qcIterations), y = eff_Mean)) +
geom_point() + geom_line(aes(group = paste(station, pressure)))
detection %>% group_by(station, pressure, order, qcIterations) %>%
summarize(TP_Mean = mean(TP_Cnt), FP_Mean = mean(FP_Cnt),
eff_Mean = mean(efficiencyRMSE))
head(detection)
cArgs = c("35121", "850", "10")
if(length(cArgs)!=3)
stop("Exactly three args are required: station, pressure and years!")
if(!as.numeric(cArgs[2]) %in% c(100,300,850))
stop("Pressure level must be in 100, 300, 850!")
if(!as.numeric(cArgs[3]) %in% c(10,20,40))
stop("Years must be in 10,20,40!")
station = as.numeric(cArgs[1])
dataset = "trh" #allowable values are "trh", "trhc", "wind", "windc"
pressure = as.numeric(cArgs[2])
startYear = 2000
endYear = startYear + as.numeric(cArgs[3]) - 1
cat("Using station",station,"\n")
cat("Using pressure",pressure,"\n")
cat("Using # of years", as.numeric(cArgs[3]))
runId = paste(cArgs, collapse="_")
cArgs = c("35121", "850", "10")
if(length(cArgs)!=3)
stop("Exactly three args are required: station, pressure and years!")
if(!as.numeric(cArgs[2]) %in% c(100,300,850))
stop("Pressure level must be in 100, 300, 850!")
if(!as.numeric(cArgs[3]) %in% c(10,20,40))
stop("Years must be in 10,20,40!")
station = as.numeric(cArgs[1])
dataset = "trh" #allowable values are "trh", "trhc", "wind", "windc"
pressure = as.numeric(cArgs[2])
startYear = 2000
endYear = startYear + as.numeric(cArgs[3]) - 1
cat("Using station",station,"\n")
cat("Using pressure",pressure,"\n")
cat("Using # of years", as.numeric(cArgs[3]))
runId = paste(cArgs, collapse="_")
if(Sys.info()[1]=="Windows" & Sys.info()[4]=="JOSH_LAPTOP"){
#   setwd("C:/Users/rockc_000/Documents/Professional Files/Mines/Research/Wind QC")
#   source("Code/simulation_functions.R")
#   source("~/GitHub/Stan-Norm-Hom-Test/snht/R/robustSNHT.R")
#   source("~/GitHub/Stan-Norm-Hom-Test/snht/R/robustSNHTunequal.R")
#   source("~/GitHub/Stan-Norm-Hom-Test/snht/R/snht.R")
setwd("C:/Users/rockc_000/Documents/Professional Files/Mines/Research/Wind QC")
source("Code/Server Code/simulation_functions.R")
source("Code/Server Code/robustSNHT.R")
source("Code/Server Code/robustSNHTunequal.R")
source("Code/Server Code/snht.R")
}
test.data = read.csv(file=paste("Data/uadb", dataset, station, "parsed_temp_cleaned.csv", sep="_") )
test.data = test.data[test.data$Pressure==pressure,]
test.data$Pressure = NULL
#Load Temperature Data
colnames(test.data)=c("Year","Month","Day","Hour","Reading")
test.data$Date = as.Date( paste(test.data$Year, test.data$Month, test.data$Day), format="%Y %m %d")
test.data$Day_Of_Year = as.numeric( as.character( test.data$Date, format="%j" ) )
test.data$Reading[test.data$Reading==-999] = NA
#Fit time series and residuals
fitTS = fitBaseTS(data=test.data)
fitST = getSkewTParams(fitTS, test.data)
test.data$Preds = predict(fitTS, newdata=test.data)
test.data$Err = test.data$Reading - test.data$Preds
ar0.5 = acfUnequal(test.data, data.col=which(colnames(test.data)=="Err"), lagScale=.5)[1,2]
## Determine completed files and start from there
files = dir(".", pattern = paste0("Simulations_.*_", runId, "\\.RData"))
simNumber = as.numeric(gsub("(Simulations_|_ch.*|_bb.*)", "", files))
completedSims = 0
if(length(simNumber) >= 1){
bestFile = files[grep(paste0(max(simNumber), "_(ch|bb)"), files)]
load(bestFile)
completedSims = nrow(dataStat)
}
#Bounds for seed are +/-2147483647 (at least on my machine)
seeds = round(runif(500 - completedSims, min=-21474836, max=21474836))
seeds
dim(sim.dat)
print("Beginning model building process...")
start = Sys.time()
seed = seeds[i]
set.seed(seed)
i = 1
seed = seeds[i]
set.seed(seed)
sim.dat = simBaseTS(fitTS, data=test.data, startYear=startYear, endYear=endYear)
sim.dat = simResiduals2(fitST, sim.dat, ar1=ar0.5^2)
orig = sim.dat
sim.dat = contaminateOutlier(sim.dat, "extreme", sample(c(0, 1,2,5,10)/100,size=1), muError=c(8,9,10))
sim.dat = contaminateBreak(sim.dat, breaks=sample(1:3*as.numeric(cArgs[3])/10,size=1), sigmaMult=c(.2,.4,.6))
contam = sim.dat
sim.dat = contaminateOutlier(sim.dat, "extreme", sample(c(0, 1,2,5,10)/100,size=1), muError=c(8,9,10))
sim.dat = contaminateBreak(sim.dat, breaks=sample(1:3*as.numeric(cArgs[3])/10,size=1), sigmaMult=c(.2,.4,.6))
sim.dat = contaminateOutlier(sim.dat, "extreme", sample(c(10)/100,size=1), muError=c(8,9,10))
sim.dat = contaminateBreak(sim.dat, breaks=sample(1:3*as.numeric(cArgs[3])/10,size=1), sigmaMult=c(.2,.4,.6))
contam = sim.dat
#Set the outlierFl now, and adjust it once outlier detection algo is run
#We do this because we need this var for the homogenization, but we don't
#always run the outlier detection before running the homogenization.
contam$outlierFl = F
toBind = data.frame( ID=seed
,outlierPct  = mean( as.numeric( sim.dat$outlierSize!=0 ) )
,outlierPct4 = mean( as.numeric( sim.dat$outlierSize==4 ) ) #% of outliers at 4 sigma
,outlierPct5 = mean( as.numeric( sim.dat$outlierSize==5 ) ) #% of outliers at 5 sigma
,outlierPct6 = mean( as.numeric( sim.dat$outlierSize==6 ) ) #% of outliers at 6 sigma
,avgTimeBtwnBreaks = mean( diff( (1:nrow(sim.dat))[sim.dat$Breaks!=0] ) )
,simBr1Size = sim.dat$Breaks[sim.dat$Breaks!=0][1]
,simBr2Size = sim.dat$Breaks[sim.dat$Breaks!=0][2]
,simBr3Size = sim.dat$Breaks[sim.dat$Breaks!=0][3]
,simBr1Time = as.Date(paste(sim.dat$Year,sim.dat$Day_Of_Year),"%Y %j")[sim.dat$Breaks!=0][1]
,simBr2Time = as.Date(paste(sim.dat$Year,sim.dat$Day_Of_Year),"%Y %j")[sim.dat$Breaks!=0][2]
,simBr3Time = as.Date(paste(sim.dat$Year,sim.dat$Day_Of_Year),"%Y %j")[sim.dat$Breaks!=0][3]
,n = nrow(sim.dat)
)
toBind
outlierType = 5
homogType = 2
order = "hFirst"
order = "oFirst"
qcIterations = 2
estimator = "huber"
sim.dat = contam
#Also remove currently computed breaks:
rm(breaks)
testNo = 1
sim.dat$outlierFl = F
#types 1, 4, and 5 all have "Tier 1"
if(outlierType==1 | outlierType==4 | outlierType==5)
{
if(estimator=="tukey")
score = tukey(sim.dat, dayWindow=365, hourBuckets=1)
else
score = huber(sim.dat, dayWindow=365, hourBuckets=1)
sim.dat$outlierFl[score>6] = T
}
#type 2 and 4 have "Tier 2"
if(outlierType==2 | outlierType==4)
{
if(estimator=="tukey")
score = tukey(sim.dat, dayWindow=45, hourBuckets=1)
else
score = huber(sim.dat, dayWindow=45, hourBuckets=1)
sim.dat$outlierFl[score>5] = T
}
#type 3 and 5 have "Hourly Tier 2"
if(outlierType==3 | outlierType==5)
{
if(estimator=="tukey")
score = tukey(sim.dat, dayWindow=45, hourBuckets=2)
else
score = huber(sim.dat, dayWindow=45, hourBuckets=2)
sim.dat$outlierFl[score>5] = T
}
sum(sim.dat$outlierFl)
if(homogType==2) out = homogenization(sim.dat = contam,
period = 365,
type = "robust",
rmPeriod = FALSE,
rmTrend = FALSE,
scaled = T,
benjamini = T,
alpha = 0.01, adj = 1)
head(contam)
sum(contam$outlierFl)
out2 = homogenization(sim.dat = sim.dat,
period = 365,
type = "robust",
rmPeriod = FALSE,
rmTrend = FALSE,
scaled = T,
benjamini = T,
alpha = 0.01, adj = 1)
head(out2)
out2[[2]]
out[[2]]
head(out[[1]])
sum(out[[1]]$outlierFl)
dim(out[[1]])
dim(out2[[1]])
infinitive = "be"
url = sprintf("http://conjugator.reverso.net/conjugation-english-verb-%s.html",
infinitive)
verbList = html(url) %>%
html_nodes("tr") %>%
xml_text()
verbList = sapply(verbList, gsub, pattern = "\r\n[[:space:]]*", replacement = " ")
library(rvest)
library(stringr)
url = sprintf("http://conjugator.reverso.net/conjugation-english-verb-%s.html",
infinitive)
verbList = html(url) %>%
html_nodes("tr") %>%
xml_text()
verbList = sapply(verbList, gsub, pattern = "\r\n[[:space:]]*", replacement = " ")
names(verbList) = NULL
charVec = verbList
stopifnot(is(charVec, "character"))
stopifnot(length(charVec) == 1)
charVec = verbList[7]
charVec
stopifnot(length(charVec) == 1)
tenseName = str_extract(" [A-Za-z]* I")
tenseName = str_extract(charVec, " [A-Za-z]* I")
tenseName
tenseName = gsub("( |I)", "", tenseName)
tenseName
indicativo = gsub("Preterite.*", "", verbList[7])
charVec = indicativo
charVec
stopifnot(is(charVec, "character"))
stopifnot(length(charVec) == 1)
tenseName = str_extract(charVec, " [A-Za-z]* I")
tenseName = gsub("( |I)", "", tenseName)
tenseName
charVec
I
I = str_extract(charVec, "I [a-zA-Z]*you")
I = str_extract(charVec, " I [a-zA-Z]*you")
I
I = gsub("( I |you )", "", I)
I
I = str_extract(charVec, " I [a-zA-Z]*you ")
I = gsub("( I |you )", "", I)
I
charVec
I = str_extract(charVec, "you [a-zA-Z]*he/she/it")
you = str_extract(charVec, "you [a-zA-Z]*he/she/it")
you
you = gsub("(you |he/she/it)", "", you)
you
he = str_extract(charVec, "he/she/it [a-zA-Z]*we")
he
he = gsub("(he/she/it |we)", "", he)
he
we = str_extract(charVec, "we [a-zA-Z]*you ")
we = gsub("(we |you )", "", we)
we
charVec
you2 = str_extract(charVec, "you [a-zA-Z]*they ")
you2 = gsub("(you |they )", "", you2)
you2
you2 = str_extract(charVec, "they [a-zA-Z]*( )*$")
you2
they = str_extract(charVec, "they [a-zA-Z]*( )*$")
they = gsub("(they| )", "", they)
they
source('~/GitHub/Italian/parseOneTenseEnglish.R', echo=TRUE)
indicativo = gsub("Preterite.*", "", verbList[7])
indicativo = parseOneTenseEnglish(indicativo)
indicativo
verbList[7]
preterite = gsub("(.*Preterite|Infinitive.*)", "", verbList[7])
preterite
preterite = parseOneTenseEnglish(preterite)
preterite
source('~/GitHub/Italian/parseOneTenseEnglish.R', echo=TRUE)
indicativo = gsub("Preterite.*", "", verbList[7])
indicativo = parseOneTenseEnglish(indicativo)
preterite = gsub("(.*Preterite|Infinitive.*)", "", verbList[7])
preterite = parseOneTenseEnglish(preterite)
indicativo
source('~/GitHub/Italian/getConjugationsEnglish.R', echo=TRUE)
preterite
charVec
verbList[7]
infinitivo = gsub("(.*Infinitive | Imperative*)", "", verbList[7])
infinitivo
infinitivo = gsub("(.*Infinitive | Imperative.*)", "", verbList[7])
infinitivo
imperative = gsub(".*Imperative ", "", verbList[7])
imperative
source("~/GitHub/Italian/runVocab.R")
vocabFile
write.csv(vocabFile, file = "Josh.csv")
verbList
names(verbList) = NULL
verbList
indicativo = gsub("Preterite.*", "", verbList[7])
indicativo = parseOneTenseEnglish(indicativo)
preterite = gsub("(.*Preterite|Infinitive.*)", "", verbList[7])
preterite = parseOneTenseEnglish(preterite)
infinitivo = gsub("(.*Infinitive | Imperative.*)", "", verbList[7])
imperative = gsub(".*Imperative ", "", verbList[7])
imperative
impForm = gsub("let's.*", "", imperative)
impForm
imperative = gsub("let's.*", "", imperative)
imperative
infinitivo
preterite
imperative = data.frame(person = c("I", "you", "he, she, it", "we", ".", "they"),
elementList = c(".", imperative, imperative,
paste("Let's", imperative), imperative, imperative))
imperative
load("~/Professional Files/Mines/Research/Wind QC/Results/Simulation_20150223/Simulations_500_bb136-01_74794_850_10.RData")
head(detectionStat)
detectionStat %>% group_by(qcIterations, order) %>%
summarize(mean(TP_Cnt), mean(FP_Cnt), mean(efficiencyRMSE))
detectionStat %>% group_by("qcIterations", "order") %>%
summarize(mean(TP_Cnt), mean(FP_Cnt), mean(efficiencyRMSE))
detectionStat %>%
summarize(mean(TP_Cnt), mean(FP_Cnt), mean(efficiencyRMSE)) %>%
group_by(qcIterations, order)
detectionStat %>% group(qcIterations, order) %>%
summarize(mean(TP_Cnt), mean(FP_Cnt), mean(efficiencyRMSE))
?group_by
detectionStat %>% group_by(qcIterations) %>%
summarize(mean(TP_Cnt), mean(FP_Cnt), mean(efficiencyRMSE))
detectionStat %>% group_by(order) %>%
summarize(mean(TP_Cnt), mean(FP_Cnt), mean(efficiencyRMSE))
colnames(detectionStat)
head(detectionStat)
grp = group_by(detectionStat, order);
summarise(grp, mean(TP_Cnt), mean(FP_Cnt), mean(efficiencyRMSE))
grp
ddply
group_by
ddply(detectionStat, by = c("order", "qcIterations"), mean)
ddply(detectionStat, by = c("order", "qcIterations"), function(df){)
ddply(detectionStat, by = c("order", "qcIterations"), function(df){
mean(TP_Cnt)
})
?ddply
ddply(detectionStat, c("order", "qcIterations"), function(df){
mean(TP_Cnt)
})
ddply(detectionStat, c("order", "qcIterations"), function(df){
mean(df$TP_Cnt)
})
ddply(detectionStat, c("order", "qcIterations"), function(df){
list(TP_Cnt = mean(df$TP_Cnt),
FP_Cnt = mean(df$FP_Cnt),
eff = mean(df$efficiencyRMSE))
})
ddply(detectionStat, c("order", "qcIterations"), function(df){
data.frame(TP_Cnt = mean(df$TP_Cnt),
FP_Cnt = mean(df$FP_Cnt),
eff = mean(df$efficiencyRMSE))
})
load("~/Professional Files/Mines/Research/Wind QC/Results/Simulation_20150223/Simulations_500_ch120-02_35121_850_10.RData")
ddply(detectionStat, c("order", "qcIterations"), function(df){
data.frame(TP_Cnt = mean(df$TP_Cnt),
FP_Cnt = mean(df$FP_Cnt),
eff = mean(df$efficiencyRMSE))
})
source("~/GitHub/Italian/runVocab.R")
dim(vocabFile)
head(vocabFile)
temp = vocabFile[, -1]
head(temp)
write.csv(temp, file = "Josh.csv", row.names = FALSE)
getStartingEstimate
